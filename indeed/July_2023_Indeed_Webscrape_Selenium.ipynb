{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55bed64",
   "metadata": {},
   "source": [
    "# `Selenium Webscraping Indeed Job Postings - July 2023`\n",
    "\n",
    "# <font color=red>Mr Fugu Data Science</font>\n",
    "\n",
    "# (◕‿◕✿)\n",
    "\n",
    "# `Purpose & Outcome:`\n",
    "\n",
    "+ Webscrape Indeed Postings\n",
    "+ Methods, drawbacks and suggestions\n",
    "+ Speeding up code and downsides with this method!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf01d1",
   "metadata": {},
   "source": [
    "# `What is Selenium and how is it used?`\n",
    "\n",
    "+ When you need to do unit testing, automation or assistance when webscraping this is a tool to aid you.\n",
    "    + Great for clicking buttons\n",
    "    + drop-down menus\n",
    "    + acting/emulating human interactions on a webpage\n",
    "  \n",
    "+ `You can use Selenium as a webscraper but, its not fast and will help if you are in a pinch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653f45f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if you have never used these: unblock the lines below to install if needed\n",
    "\n",
    "# !pip install webdriver-manager\n",
    "# !pip3 install lxml\n",
    "# !pip3 install selenium\n",
    "# !pip3 install webdriver_manager\n",
    "# !pip install --upgrade pip\n",
    "# !pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d754db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- import necessary modules -------\n",
    "\n",
    "# For webscraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parsing and creating xml data\n",
    "from lxml import etree as et\n",
    "\n",
    "# Store data as a csv file written out\n",
    "from csv import writer\n",
    "\n",
    "# In general to use with timing our function calls to Indeed\n",
    "import time\n",
    "\n",
    "# Assist with creating incremental timing for our scraping to seem more human\n",
    "from time import sleep\n",
    "\n",
    "# Dataframe stuff\n",
    "import pandas as pd\n",
    "\n",
    "# Random integer for more realistic timing for clicks, buttons and searches during scraping\n",
    "from random import randint\n",
    "\n",
    "# Multi Threading\n",
    "import threading\n",
    "\n",
    "# Threading:\n",
    "from concurrent.futures import ThreadPoolExecutor, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bee43e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.15.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import selenium\n",
    "\n",
    "# Check version I am running\n",
    "selenium.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba65a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium 4:\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "# Starting/Stopping Driver: can specify ports or location but not remote access\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "\n",
    "# Manages Binaries needed for WebDriver without installing anything directly\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f41e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows searchs similar to beautiful soup: find_all\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Try to establish wait times for the page to load\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# Wait for specific condition based on defined task: web elements, boolean are examples\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Used for keyboard movements, up/down, left/right,delete, etc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Locate elements on page and throw error if they do not exist\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c820c",
   "metadata": {},
   "source": [
    "# `Consider Headless Browser: speed up & uses less resources`\n",
    "\n",
    "There are some condiserations though:\n",
    "\n",
    "+ Some browsers create issues\n",
    "+ debugging can be tricky\n",
    "+ you may have limited plugin usage or support\n",
    "+ you are not able to see visually how the website or application are working \n",
    "\n",
    "`-------------------------------------------------`\n",
    "\n",
    "# `from selenium.webdriver.common.by import By`\n",
    "\n",
    "Think of this as being similar to using `Beautiful Soup and find_all`\n",
    "+ when used it allows you to find something within an HTML document, if it fails you raise the exception: `NoSuchElementException`\n",
    "+ **`Becareful when using BY`** because if this is not a static page then any attrubutes you are searching can become an error in the future when it fails.\n",
    "    + For example if you are searching by `Class` this can create issues later vs using\n",
    "        + This is because it is a `CSS` selector and can change overtime since it is an attribute\n",
    "    + `ID` which may make your code more robust! This CAN be a unique identifier that may help you instead\n",
    "\n",
    "# `NoSuchElementException`\n",
    "\n",
    "This is useful to locate elements within a page while loading and try to handle exceptions.\n",
    "+ During `AJAX` calls you may have issues if the application was build using `React, VUE, Angular` and require different use cases to make the above checks. [article to explain](https://reflect.run/articles/everything-you-need-to-know-about-nosuchelementexception-in-selenium/) and you can consider polling.\n",
    "\n",
    "`-------------------------------------------------`\n",
    "\n",
    "# `Other Common Errors:`\n",
    "\n",
    "+ **`InvalidSelectorException`**\n",
    "\n",
    "+ **`ElementNotInteractableException`**\n",
    "\n",
    "+ **`TimeoutException`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d6c62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows you to cusotmize: ingonito mode, maximize window size, headless browser, disable certain features, etc\n",
    "option= webdriver.ChromeOptions()\n",
    "\n",
    "# Going undercover:\n",
    "option.add_argument(\"--incognito\")\n",
    "\n",
    "\n",
    "# # Consider this if the application works and you know how it works for speed ups and rendering!\n",
    "\n",
    "# option.add_argument('--headless=chrome')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae3a9129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define job and location search keywords\n",
    "job_search_keyword = ['Data+Scientist', 'Business+Analyst', 'Data+Engineer', \n",
    "                      'Python+Developer', 'Full+Stack+Developer', \n",
    "                      'Machine+Learning+Engineer']\n",
    "\n",
    "# Define Locations of Interest\n",
    "location_search_keyword = ['New+York', 'California', 'Washington']\n",
    "\n",
    "# Finding location, position, radius=35 miles, sort by date and starting page\n",
    "paginaton_url = 'https://www.indeed.com/jobs?q={}&l={}&radius=35&filter=0&sort=date&start={}'\n",
    "\n",
    "# print(paginaton_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30276a3f",
   "metadata": {},
   "source": [
    "# `Things to consider when scraping data:`\n",
    "\n",
    "+ Wait for page to load before we start running tasks\n",
    "+ make sure what we are looking for is actually there\n",
    "    + It can be absent\n",
    "    + hidden in DOM, iframe or similar\n",
    "+ timing our calls to remain more like an average user\n",
    "+ Exception handling\n",
    "\n",
    "`----------------------------------------------`\n",
    "\n",
    "# `I/O vs CPU Bound:`\n",
    "\n",
    "**`During webscraping tasks you are I/O bound!`** you are making calls to retreive `HTML`. Try to avoid unnecessary calls which may get your IP Address blocked like I have many times. [CPU, I/O article](https://testdriven.io/blog/concurrency-parallelism-asyncio/)\n",
    "\n",
    "+ **`Multi-Threading:`** `concurrent`\n",
    "    + Your tasks will not run parrallel here and they run one after another. \n",
    "    + If something is waiting or slow it can start working on another task and will be asynchronous\n",
    "        + Meaning that you can have tasks out of order and not 1-2-3 but maybe 0-2-3-1 for example of order\n",
    "        + This can occur due to Network or I/O operations\n",
    "+ **`Multi-Processing:`** `parrallel`\n",
    "\n",
    "+ **`AsyncIO:`** benefit of threading but not worrying about wait times and running more tasks during a wait time.\n",
    "This is a step above the threading from above but requires more code and thought to setup.\n",
    "\n",
    "`----------------------------------------------`\n",
    "\n",
    "+ **`Asynchronous:`** think of running one task and then calling the next task before the first task has finished. This happens when you send a response but don't receive an answer so you go to the next person in line and when you are free and have a response from prior person you then go ahead and help them. Essentially, lowering the idle time of waiting for a response. [Good breakdown and visuals](https://medium.com/analytics-vidhya/asynchronous-web-scraping-101-fetching-multiple-urls-using-arsenic-ec2c2404ecb4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf0119",
   "metadata": {},
   "source": [
    "# `Let's look at what is going on below:`\n",
    "\n",
    "There are some concerns and things for you to consider:\n",
    "\n",
    "1.) Below is the MAX number of jobs you will find for a posting of interest!\n",
    "\n",
    "2.) This is not an accurate depiction because you can have way less than this depending on results\n",
    "    \n",
    "    + An issue arises due to duplicated listings\n",
    "\n",
    "3.) Pagination is difficult to do and when to stop the search results\n",
    "\n",
    "4.) you have a filter option (&filter=0, &filter=1), filter =1 shows non-duplicates which reduces results but you need to figure out how to do pagination!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef86145",
   "metadata": {},
   "source": [
    "`---------------------------------------------------------------`\n",
    "\n",
    "# `First let's try to find number of jobs for a given posting`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d8435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.96416687965393 seconds to complete action!\n",
      "-----------------------\n",
      "Max Iterable Pages for this search: 13\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "job_='Data+Engineer'\n",
    "location='Washington'\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),\n",
    "                         options=option)\n",
    "\n",
    "\n",
    "driver.get(paginaton_url.format(job_,location,0))\n",
    "\n",
    "# t = ScrapeThread(url_)\n",
    "# t.start()\n",
    "\n",
    "sleep(randint(2, 6))\n",
    "\n",
    "p=driver.find_element(By.CLASS_NAME,'jobsearch-JobCountAndSortPane-jobCount').text\n",
    "\n",
    "# Max number of pages for this search! There is a caveat described soon\n",
    "max_iter_pgs=int(p.split(' ')[0])//15 \n",
    "\n",
    "\n",
    "driver.quit() # Closing the browser we opened\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start,'seconds to complete action!')\n",
    "print('-----------------------')\n",
    "print('Max Iterable Pages for this search:',max_iter_pgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da6c73",
   "metadata": {},
   "source": [
    " \n",
    "`----------------------------------------------------------` \n",
    " \n",
    "# Notes for this project:\n",
    "\n",
    "+ Filling in forms:\n",
    "+ click buttons\n",
    "+ possible human detection stuff\n",
    "\n",
    "**`Xpath vs CSS selectors for retreiving data`**\n",
    "\n",
    "+ `Xpath:` bidirectional (can go from parent to child and reverse) traversal\n",
    "    + slower retrevial speed\n",
    "    + text functions supported\n",
    "    + pay attention to relative '//' and absolute path '/' notations\n",
    "    + Think of a tree like structure to breakdown\n",
    "+ `CSS:` directional (parent to child only)\n",
    "\n",
    "`------------------------`\n",
    "\n",
    "**`Xpath`**\n",
    "+ *`Xpath`* stands for `XML Path` which is a query language used to find the path of an element in XML documents\n",
    "+ Essentially you are navigating a `DOM` \n",
    "+ More flexible than using `CSS`\n",
    "    + If you don't know the name of an element you can use `contains` as your key word which is great!\n",
    " \n",
    "**`CSS`**\n",
    "+ Most often the HTML will be styled in a cascading format and identifying elements will come from the `Class` they fall within\n",
    "+ They are used to select various elements within a `DOM`\n",
    "    + **`Simple selectors:`** such as finding a `Class` or `ID`\n",
    "    + **`Attribute selectors:`** \n",
    "    + **`Pseudo selectors:`** such as hover boxes or check boxes as examples\n",
    "    \n",
    "# `Wait times: ` because of how webpages are rendered you will/can have various items loading at different times. \n",
    "This can be a problem when you are webscraping. If you try to grab the elements too fast you can miss something or \n",
    "cause errors to occur which could have been avoided. \n",
    "\n",
    "Ways to combat this can include explicit waits within Selenium such as [selenium doc](https://selenium-python.readthedocs.io/waits.html) \n",
    "\n",
    "`from selenium.webdriver.support.wait import WebDriverWait`\n",
    "\n",
    "`from selenium.webdriver.support import expected_conditions as EC`\n",
    "\n",
    "`----------------------------------------------------------------------`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a53efc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".companyName\"}\n  (Session info: chrome=120.0.6099.130); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x006F6EE3+174339]\n\t(No symbol) [0x00620A51]\n\t(No symbol) [0x00336FF6]\n\t(No symbol) [0x00369876]\n\t(No symbol) [0x00369C2C]\n\t(No symbol) [0x00362631]\n\t(No symbol) [0x00387054]\n\t(No symbol) [0x003625B0]\n\t(No symbol) [0x00387414]\n\t(No symbol) [0x0039A104]\n\t(No symbol) [0x00386DA6]\n\t(No symbol) [0x00361034]\n\t(No symbol) [0x00361F8D]\n\tGetHandleVerifier [0x00794B1C+820540]\n\tsqlite3_dbdata_init [0x008553EE+653550]\n\tsqlite3_dbdata_init [0x00854E09+652041]\n\tsqlite3_dbdata_init [0x008497CC+605388]\n\tsqlite3_dbdata_init [0x00855D9B+656027]\n\t(No symbol) [0x0062FE6C]\n\t(No symbol) [0x006283B8]\n\t(No symbol) [0x006284DD]\n\t(No symbol) [0x00615818]\n\tBaseThreadInitThunk [0x7692FCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x775E7C6E+286]\n\tRtlGetAppContainerNamedObjectPath [0x775E7C3E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 46\u001b[0m\n\u001b[0;32m     33\u001b[0m         job_title \u001b[38;5;241m=\u001b[39m jj\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjobTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#         print(job_title.text)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         \n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Href's to get full job description (need to re-terate to get full info)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Posting date\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Job description\u001b[39;00m\n\u001b[0;32m     43\u001b[0m         job_lst\u001b[38;5;241m.\u001b[39mappend([job_title\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m     44\u001b[0m         job_title\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     45\u001b[0m         job_title\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m),      \n\u001b[1;32m---> 46\u001b[0m         \u001b[43mjj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompanyName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext,       \n\u001b[0;32m     47\u001b[0m         jj\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompanyLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m     48\u001b[0m         jj\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m     49\u001b[0m         job_title\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget_attribute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;66;03m# I removed the metadata attached to this class name to work!\u001b[39;00m\n\u001b[0;32m     53\u001b[0m             salary_list\u001b[38;5;241m.\u001b[39mappend(jj\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msalary-snippet-container\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32mc:\\Users\\kelvin\\envs\\job_scrapers\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:417\u001b[0m, in \u001b[0;36mWebElement.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    414\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    415\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_CHILD_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\kelvin\\envs\\job_scrapers\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py:395\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    393\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    394\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kelvin\\envs\\job_scrapers\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:348\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    346\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 348\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\kelvin\\envs\\job_scrapers\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".companyName\"}\n  (Session info: chrome=120.0.6099.130); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x006F6EE3+174339]\n\t(No symbol) [0x00620A51]\n\t(No symbol) [0x00336FF6]\n\t(No symbol) [0x00369876]\n\t(No symbol) [0x00369C2C]\n\t(No symbol) [0x00362631]\n\t(No symbol) [0x00387054]\n\t(No symbol) [0x003625B0]\n\t(No symbol) [0x00387414]\n\t(No symbol) [0x0039A104]\n\t(No symbol) [0x00386DA6]\n\t(No symbol) [0x00361034]\n\t(No symbol) [0x00361F8D]\n\tGetHandleVerifier [0x00794B1C+820540]\n\tsqlite3_dbdata_init [0x008553EE+653550]\n\tsqlite3_dbdata_init [0x00854E09+652041]\n\tsqlite3_dbdata_init [0x008497CC+605388]\n\tsqlite3_dbdata_init [0x00855D9B+656027]\n\t(No symbol) [0x0062FE6C]\n\t(No symbol) [0x006283B8]\n\t(No symbol) [0x006284DD]\n\t(No symbol) [0x00615818]\n\tBaseThreadInitThunk [0x7692FCC9+25]\n\tRtlGetAppContainerNamedObjectPath [0x775E7C6E+286]\n\tRtlGetAppContainerNamedObjectPath [0x775E7C3E+238]\n"
     ]
    }
   ],
   "source": [
    "# Pagination: PRACTICE\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "job_='Data+Engineer'\n",
    "location='Washington'\n",
    "\n",
    "\n",
    "job_lst=[]\n",
    "job_description_list_href=[]\n",
    "\n",
    "# job_description_list = []\n",
    "salary_list=[]\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),\n",
    "                         options=option)\n",
    "sleep(randint(2, 6))\n",
    "\n",
    "# driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
    "\n",
    "for i in range(0,max_iter_pgs):\n",
    "    driver.get(paginaton_url.format(job_,location,i*10))\n",
    "    \n",
    "    \n",
    "    sleep(randint(2, 4))\n",
    "\n",
    "    job_page = driver.find_element(By.ID,\"mosaic-jobResults\")\n",
    "    jobs = job_page.find_elements(By.CLASS_NAME,\"job_seen_beacon\") # return a list\n",
    "\n",
    "    for jj in jobs:\n",
    "        job_title = jj.find_element(By.CLASS_NAME,\"jobTitle\")\n",
    "#         print(job_title.text)\n",
    "        \n",
    "# Href's to get full job description (need to re-terate to get full info)\n",
    "# Reference ID for each job used by indeed         \n",
    "# Finding the company name        \n",
    "# Location\n",
    "# Posting date\n",
    "# Job description\n",
    "\n",
    "        job_lst.append([job_title.text,\n",
    "        job_title.find_element(By.CSS_SELECTOR,\"a\").get_attribute(\"href\"),\n",
    "        job_title.find_element(By.CSS_SELECTOR,\"a\").get_attribute(\"id\"),      \n",
    "        jj.find_element(By.CLASS_NAME,\"companyName\").text,       \n",
    "        jj.find_element(By.CLASS_NAME,\"companyLocation\").text,\n",
    "        jj.find_element(By.CLASS_NAME,\"date\").text,\n",
    "        job_title.find_element(By.CSS_SELECTOR,\"a\").get_attribute(\"href\")])\n",
    "        \n",
    "\n",
    "        try: # I removed the metadata attached to this class name to work!\n",
    "            salary_list.append(jj.find_element(By.CLASS_NAME,\"salary-snippet-container\").text)\n",
    "\n",
    "        except NoSuchElementException: \n",
    "            try: \n",
    "                salary_list.append(jj.find_element(By.CLASS_NAME,\"estimated-salary\").text)\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                salary_list.append(None)\n",
    "      \n",
    "                \n",
    "#         # Click the job element to get the description\n",
    "#         job_title.click()\n",
    "        \n",
    "#         # Help to load page so we can find and extract data\n",
    "#         sleep(randint(3, 5))\n",
    "\n",
    "#         try: \n",
    "#             job_description_list.append(driver.find_element(By.ID,\"jobDescriptionText\").text)\n",
    "            \n",
    "#         except: \n",
    "            \n",
    "#             job_description_list.append(None)\n",
    "\n",
    "driver.quit() \n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start,'seconds to complete Query!')\n",
    "\n",
    "# alternate way to grab the info for job description to make it faster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc6cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49414e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_lst[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fb0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_list[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf38ec",
   "metadata": {},
   "source": [
    "# `Here is a side note:`\n",
    "\n",
    "\n",
    "+ This gives me an error because it was code from the past version:\n",
    "\n",
    "`driver = webdriver.Chrome(ChromeDriverManager().install())`\n",
    "\n",
    "\n",
    "+ `When using ingonito browser:` your browsing tabs will pull different data than a normal window. Understand this when doing your troubleshooting and debugging. If you have a window open to find your tags but parse in a different type of window the results will not line up.\n",
    "\n",
    "+ Also, when you are grabbing `job descriptions` for example you will need to time it so the page will read the data after it is loaded. If you immediately try to grab data you may not get everything!\n",
    "    + Option 1: use the clickable tab from the `job title` then scrape directly\n",
    "    + Option 2: consider saving the `HREF's` and then doing a separate parsing in a different function. This I think may be faster. But, check for yourself.\n",
    "    \n",
    "+ To speed things up consider `headless browser` but, understand the debugging becomes an issue!\n",
    "\n",
    "+ **If you parse a good amount of pages** you will encounter a checkbox that needs to be clicked to show you are not a robot. This occurs to me usually after 15-30 pages of scraping which is not a lot. (I need to figure this out)\n",
    "    + Option 1: try to see if you can pull the information for this button to scrape it directly and click\n",
    "    + Option 2: reset and tinker with the settings of timing out, sleep settings and maybe error handling\n",
    "\n",
    "**`Big Concern: Pagination`**\n",
    "When you need to go from page to page sequentially this is not straight forward. Practice and a lot of reading will aid you. I am not savvy just yet.\n",
    "+ Clickable buttons and learning how to use them and WHEN TO STOP iterating are NOT trivial tasks\n",
    "+ Hacking your way through, such I did for this example but, there is a glaring issue with duplicate entries.\n",
    "+ Finding hidden elements and figuring out how to extract them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa1863",
   "metadata": {},
   "source": [
    "# `Option 1 Find Description Links From Beginning:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c392044",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),\n",
    "                         options=option)\n",
    "sleep(randint(2, 6))\n",
    "\n",
    "\n",
    "for i in range(0,max_iter_pgs):\n",
    "    driver.get(paginaton_url.format(job_,location,i*10))\n",
    "    \n",
    "    sleep(randint(2, 4))\n",
    "\n",
    "    job_page = driver.find_element(By.ID,\"mosaic-jobResults\")\n",
    "    jobs = job_page.find_elements(By.CLASS_NAME,\"job_seen_beacon\") # return a list\n",
    "\n",
    "    for jj in jobs:\n",
    "        job_title = jj.find_element(By.CLASS_NAME,\"jobTitle\")\n",
    "\n",
    "                \n",
    "        # Click the job element to get the description\n",
    "        job_title.click()\n",
    "        \n",
    "        # Help to load page so we can find and extract data\n",
    "        sleep(randint(3, 5))\n",
    "\n",
    "        try: \n",
    "            job_description_list.append(driver.find_element(By.ID,\"jobDescriptionText\").text)\n",
    "            \n",
    "        except: \n",
    "            \n",
    "            job_description_list.append(None)\n",
    "driver.quit()\n",
    "# job_description_list[-17:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "job_description_list_02=[]\n",
    "descr_link_lst=[]\n",
    "for descr_link in range(len(job_lst)):\n",
    "    descr_link_lst.append(job_lst[descr_link][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a699f",
   "metadata": {},
   "source": [
    "# `Option 2: call links from list, iterate links directly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f18c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headless browser\n",
    "# possible wait function for page to load\n",
    "\n",
    "# import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "for link in descr_link_lst:\n",
    "    option_= webdriver.ChromeOptions()\n",
    "\n",
    "# Going undercover:\n",
    "    option_.add_argument(\"--incognito\")\n",
    "    \n",
    "    option_.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),\n",
    "                         options=option_)\n",
    "    driver.get(link)\n",
    "#     job_page = driver.find_element(By.ID,\"mosaic-jobResults\")\n",
    "#     jobs = job_page.find_elements(By.CLASS_NAME,\"job_seen_beacon\") # return a list\n",
    "    sleep(randint(2, 5))\n",
    "    try: \n",
    "        job_description_list_02.append(driver.find_element(By.ID,\"jobDescriptionText\").text)\n",
    "#         print(driver.find_element(By.ID,\"jobDescriptionText\").text)   \n",
    "    except: \n",
    "            \n",
    "        job_description_list_02.append(None)\n",
    "    driver.quit()\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3858ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description from 2nd to last entry as illustrate\n",
    "job_description_list_02[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to do Threading for speed up:\n",
    "\n",
    "# import threading\n",
    "# from selenium import webdriver\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "class ScrapeThread(threading.Thread):\n",
    "    def __init__(self, url):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.url = url\n",
    "\n",
    "threads = []\n",
    "for url in descr_link_lst:\n",
    "    option_= webdriver.ChromeOptions()\n",
    "\n",
    "# Going undercover:\n",
    "    option_.add_argument(\"--incognito\")\n",
    "    \n",
    "    option_.add_argument(\"--headless=new\")\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),\n",
    "                         options=option_)\n",
    "    driver.get(url)\n",
    "    t = ScrapeThread(url)\n",
    "    t.start()\n",
    "    \n",
    "    try: \n",
    "        threads.append(driver.find_element(By.ID,\"jobDescriptionText\").text)\n",
    " \n",
    "    except: \n",
    "        threads.append(None)\n",
    "        \n",
    "driver.quit()\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878171e5",
   "metadata": {},
   "source": [
    "# `Why I cannot use Beautiful Soup ANYMORE.. Let's talk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11364bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for url_link in descr_link_lst:\n",
    "# job_descr_txt=[]    \n",
    "# # headers=headers\n",
    "# url_1='https://www.indeed.com/jobs?q={}&l={}&radius=35&filter=0&sort=date'\n",
    "# response = requests.get(url_1.format('data+engineer','denver'))\n",
    "# # ,headers=headers)\n",
    "# print(response)\n",
    "# html_ = response.text\n",
    "# # print(html_)\n",
    "# soup_ = BeautifulSoup(html_, 'html.parser')\n",
    "# print(soup_.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c683717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short Version to show illustration:\n",
    "\n",
    "paginaton_url_ = 'https://www.indeed.com/jobs?q={}&l={}&sort=date&start={}'\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),\n",
    "                         options=option)\n",
    "p_=[]\n",
    "salary_list_=[]\n",
    "for i in range(0,3):\n",
    "    driver.get(paginaton_url_.format(job_,location,i*10))\n",
    "    sleep(randint(2, 3))\n",
    "    \n",
    "    job_page = driver.find_element(By.ID,\"mosaic-jobResults\")\n",
    "    jobs = job_page.find_elements(By.CLASS_NAME,\"job_seen_beacon\") # return a list\n",
    "    \n",
    "    for jj in jobs:\n",
    "        job_title = jj.find_element(By.CLASS_NAME,\"jobTitle\")\n",
    "        print(job_title.text)\n",
    "        p_.append(job_title.text)\n",
    "#         sleep(randint(3, 5))\n",
    "        try:\n",
    "            salary_list_.append(jj.find_element(By.CLASS_NAME,\"salary-snippet-container\").text)\n",
    "            print(jj.find_element(By.CLASS_NAME,\"salary-snippet-container\").text)\n",
    "\n",
    "        except: \n",
    "            try: \n",
    "                salary_list.append(jj.find_element(By.CLASS_NAME,\"estimated-salary\").text)\n",
    "                print(jj.find_element(By.CLASS_NAME,\"estimated-salary\").text)\n",
    "            except:\n",
    "                print('None')\n",
    "                \n",
    "driver.quit()\n",
    "\n",
    "# //*[@id=\"challenge-stage\"]/div/label/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7521a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df and store data\n",
    "\n",
    "\n",
    "# duplicate entries remove!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c045340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b6cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider NLP\n",
    "\n",
    "\n",
    "# class ScrapeThread(threading.Thread):\n",
    "#     def __init__(self, url):\n",
    "#         threading.Thread.__init__(self)\n",
    "#         self.url = url\n",
    "\n",
    "        \n",
    "        \n",
    "# multi-thread or asynio\n",
    "# explicit wait with Ec.wait read this and above explainations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831d748",
   "metadata": {},
   "source": [
    "# `Future improvements for this work:`\n",
    "\n",
    "+ Speed up: use asyncio and also look at threading to explain differences\n",
    "+ clean up data, put into DF and do some plotting\n",
    "+ consider more than one job type after speed up\n",
    "+ look into explicit wait times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e00647",
   "metadata": {},
   "source": [
    "# Like, Share & <font color=red>SUB</font>scribe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebeef3e",
   "metadata": {},
   "source": [
    "# `Citations & Help:`\n",
    "\n",
    "# ◔̯◔\n",
    "\n",
    "https://pypi.org/project/webdriver-manager/\n",
    "\n",
    "https://www.blog.datahut.co/post/scrape-indeed-using-selenium-and-beautifulsoup\n",
    "\n",
    "https://github.com/henrionantony/Dynamic-Web-Scraping-using-Python-and-Selenium/blob/master/indeed.py\n",
    "\n",
    "https://www.specrom.com/blog/web-scraping-job-postings-on-indeed-using-python/\n",
    "\n",
    "https://www.scrapingdog.com/blog/scrape-indeed-using-python/ (bs4 as of Feb 13, 2023)\n",
    "\n",
    "https://selenium-python.readthedocs.io/locating-elements.html#locating-elements\n",
    "\n",
    "https://stackoverflow.com/questions/50865088/how-to-get-string-dump-of-lxml-element\n",
    "\n",
    "https://selenium-python.readthedocs.io/navigating.html\n",
    "\n",
    "https://towardsdatascience.com/web-scraping-job-postings-from-indeed-com-using-selenium-5ae58d155daf (2020 version)\n",
    "\n",
    "https://www.pycodemates.com/2022/01/Indeed-jobs-scraping-with-python-bs4-selenium-and-pandas.html\n",
    "\n",
    "https://medium.com/forcodesake/how-to-build-a-scraping-tool-for-indeed-in-8-minutes-data-science-csv-selenium-beautifulsoup-python-95fcca4b9719 (Good Read & Adapted Code)\n",
    "\n",
    "https://www.tutorialspoint.com/how-to-open-browser-window-in-incognito-private-mode-using-python-selenium-webdriver\n",
    "\n",
    "https://www.selenium.dev/selenium/docs/api/py/webdriver/selenium.webdriver.common.keys.html\n",
    "\n",
    "https://pythonbasics.org/selenium-wait-for-page-to-load/\n",
    "\n",
    "https://www.seleniumeasy.com/selenium-tutorials/selenium-headless-browser-execution\n",
    "\n",
    "https://www.browserstack.com/guide/expectedconditions-in-selenium\n",
    "\n",
    "https://www.testim.io/blog/xpath-vs-css-selector-difference-choose/\n",
    "\n",
    "https://www.w3.org/TR/REC-DOM-Level-1/introduction.html\n",
    "\n",
    "https://github.com/diego-florez/Selenium-Web-Scraping/blob/master/indeed.py (Indeed scrape Selenium 2020) error Handling also\n",
    "\n",
    "https://www.testim.io/blog/selenium-click-button/\n",
    "\n",
    "https://scrapfly.io/blog/how-to-scrape-indeedcom/\n",
    "\n",
    "https://goh.physics.ucdavis.edu/datascience/webscraping/webscraping.html\n",
    "\n",
    "https://levelup.gitconnected.com/efficiently-scraping-multiple-pages-of-data-a-guide-to-handling-pagination-with-selenium-and-3ed93857f596\n",
    "\n",
    "https://github.com/israel-dryer/Indeed-Job-Scraper/blob/master/indeed-job-scraper-selenium.ipynb\n",
    "\n",
    "https://www.zenrows.com/blog/headless-browser-python#switch-to-python-selenium-headless-mode\n",
    "\n",
    "https://python.plainenglish.io/pagination-techniques-to-scrape-data-from-any-website-in-python-779cd32bd514\n",
    "\n",
    "https://www.selenium.dev/blog/2023/headless-is-going-away/ (2023 article)\n",
    "\n",
    "https://www.zenrows.com/blog/bypass-cloudflare-python (cloudflare bot blocking 403 error)\n",
    "\n",
    "`Code Optimizing with Asynio, multi-threading and multi-processing:`\n",
    "\n",
    "https://www.geeksforgeeks.org/multithreading-or-multiprocessing-with-python-and-selenium/\n",
    "\n",
    "https://www.youtube.com/watch?v=-hw3AaxX5B4\n",
    "\n",
    "https://webnus.net/how-to-speed-up-selenium-automated-tests-in-2022/ (selenium speed up ideas)\n",
    "\n",
    "https://medium.com/@PhysicistMarianna/scrape-job-postings-data-from-indeed-com-with-python-b4f31340ef5f (bs4 help maybe)\n",
    "\n",
    "https://github.com/Ram-95/Indeed_Job_Scraper/blob/master/Indeed_Job_Scraper.py (bs4 idea as well)\n",
    "\n",
    "https://www.youtube.com/watch?v=HOS5Hix--bE\n",
    "\n",
    "https://stackoverflow.com/questions/75849391/failed-to-fetch-the-job-titles-from-indeed-using-the-requests-module (cloudscraper idea)\n",
    "\n",
    "https://www.geeksforgeeks.org/multithreading-python-set-1/ (multi-threading ex.)\n",
    "\n",
    "https://testdriven.io/blog/building-a-concurrent-web-scraper-with-python-and-selenium/ (come back to this! good write up with code...)\n",
    "\n",
    "https://medium.com/analytics-vidhya/asynchronous-web-scraping-101-fetching-multiple-urls-using-arsenic-ec2c2404ecb4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f9aeda",
   "metadata": {},
   "source": [
    "# Notes for this project:\n",
    "\n",
    "+ Filling in forms:\n",
    "+ click buttons\n",
    "+ possible human detection stuff\n",
    "\n",
    "**`Xpath vs CSS selectors for retreiving data`**\n",
    "\n",
    "+ `Xpath:` bidirectional (can go from parent to child and reverse) traversal\n",
    "    + slower retrevial speed\n",
    "    + text functions supported\n",
    "    + pay attention to relative '//' and absolute path '/' notations\n",
    "    + Think of a tree like structure to breakdown\n",
    "+ `CSS:` directional (parent to child only)\n",
    "\n",
    "`------------------------`\n",
    "\n",
    "**`Xpath`**\n",
    "+ *`Xpath`* stands for `XML Path` which is a query language used to find the path of an element in XML documents\n",
    "+ Essentially you are navigating a `DOM` \n",
    "+ More flexible than using `CSS`\n",
    "    + If you don't know the name of an element you can use `contains` as your key word which is great!\n",
    " \n",
    "**`CSS`**\n",
    "+ Most often the HTML will be styled in a cascading format and identifying elements will come from the `Class` they fall within\n",
    "+ They are used to select various elements within a `DOM`\n",
    "    + **`Simple selectors:`** such as finding a `Class` or `ID`\n",
    "    + **`Attribute selectors:`** \n",
    "    + **`Pseudo selectors:`** such as hover boxes or check boxes as examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
