{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# create a webdriver instance\n",
    "# latest headless options: https://stackoverflow.com/questions/46753393/how-to-run-headless-firefox-with-selenium-in-python\n",
    "options = Options()\n",
    "options.add_argument(\"-headless\")\n",
    "options.add_argument(\"-privatebrowsing\")\n",
    "\n",
    "driver = webdriver.Firefox(options=options)\n",
    "# print(\"Headless Firefox Initialized\")\n",
    "\n",
    "# Opening linkedIn's login page\n",
    "driver.get(\"https://linkedin.com/uas/login\")\n",
    "# driver.get(\"http://google.com/\")\n",
    " \n",
    "# waiting for the page to load\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login To LinkedIn using provided credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering username\n",
    "_USERNAME = driver.find_element(By.ID, \"username\")\n",
    " \n",
    "# Enter Your Email Address\n",
    "_USERNAME.send_keys(os.getenv('USERNAME'))  \n",
    " \n",
    "# entering password\n",
    "_PASSWORD = driver.find_element(By.ID, \"password\")\n",
    " \n",
    "# Enter Your Password\n",
    "_PASSWORD.send_keys(os.getenv('PASSWORD'))\n",
    "\n",
    "# Clicking on the log in button\n",
    "# Format (syntax) of writing XPath --> \n",
    "# //tagname[@attribute='value']\n",
    "driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    "\n",
    "# waiting for the page to load\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bypass Job search Inputs and use user provided base url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.get(\"https://www.linkedin.com/jobs/search/\")\n",
    "# # jobs.click()\n",
    "\n",
    "# wait = WebDriverWait(driver, 10)\n",
    "# wait.until(EC.url_changes('https://www.linkedin.com/jobs/search/'))\n",
    "# get_url = driver.current_url\n",
    "# print(\"The current url is:\"+str(get_url))\n",
    "\n",
    "# # waiting for the page to load\n",
    "# time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # entering username\n",
    "# TITLE_SKILL_COMPANY_INPUT = driver.find_element(By.XPATH, \"//*[@id='jobs-search-box-keyword-id-ember142']\")\n",
    " \n",
    "# # Enter Your Email Address\n",
    "# TITLE_SKILL_COMPANY_INPUT.send_keys(os.getenv('TITLE_SKILL_COMPANY'))  \n",
    " \n",
    "# # entering password\n",
    "# LOCATION_INPUT = driver.find_element(By.XPATH, \"//*[@id='jobs-search-box-location-id-ember142']\")\n",
    " \n",
    "# # Enter Your Password\n",
    "# LOCATION_INPUT.send_keys(os.getenv('LOCATION'))\n",
    "\n",
    "# # Clicking on the log in button\n",
    "# # Format (syntax) of writing XPath --> \n",
    "# # //tagname[@attribute='value']\n",
    "# driver.find_element(By.CLASS_NAME, \"jobs-search-box__submit-button\").click()\n",
    "\n",
    "# # waiting for the page to load\n",
    "# time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to user provided url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(os.getenv(\"USER_PROVIDED_URL\"))\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save first page for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_page = driver.page_source\n",
    " \n",
    "# soup = BeautifulSoup(first_page, 'lxml')\n",
    "# # print(soup)\n",
    "# with open(\"first-page.html\", \"w\", encoding='utf-8') as file:\n",
    "#     file.write(str(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract information from first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('first_page.html', 'r', encoding='utf-8') as f:\n",
    "    # contents = f.read()\n",
    "\n",
    "first_page = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(first_page, 'lxml')\n",
    "num_results_div = soup.find(\"div\", attrs={\"class\": \"jobs-search-results-list__subtitle\"})\n",
    "num_results_text = num_results_div.span.get_text(strip=True)\n",
    "int_num_results = int(num_results_text.split()[0])\n",
    "assert isinstance(int_num_results, int) and int_num_results > 0\n",
    "\n",
    "int_num_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 100 jobs from save state.\n"
     ]
    }
   ],
   "source": [
    "job_post_ids=set()\n",
    "output_json = \"sample.json\"\n",
    "max_num = 0 # loose approximation for number of jobs to scrape\n",
    "\n",
    "try:\n",
    "  with open(output_json) as infile:\n",
    "    job_desc_id = json.load(infile)\n",
    "\n",
    "  print(f\"Retrieved {len(job_desc_id)} jobs from save state.\")\n",
    "\n",
    "except OSError as e:\n",
    "  print(f\"Previous save state: {output_json} not found: {e}\")\n",
    "  job_desc_id = {}    # job_id: {job_title: x, job_desc: y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch all job post ids from first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _base_url = os.getenv(\"USER_PROVIDED_URL\")\n",
    "# # _postfix = \"&start=25\"\n",
    "# # page_url = _base_url + _postfix\n",
    "\n",
    "# driver.get(_base_url)\n",
    "# time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with open('first_page.html', 'r', encoding='utf-8') as f:\n",
    "\n",
    "# #     contents = f.read()\n",
    "\n",
    "# #     soup = BeautifulSoup(contents, 'lxml')\n",
    "# #     job_post_divs = soup.find_all('div', attrs={\"data-view-name\": \"job-card\"})\n",
    "\n",
    "# #     for div in job_post_divs:\n",
    "# #         job_post_ids.append(div[\"data-job-id\"])\n",
    "\n",
    "# #     job_post_lis = soup.find_all('li', attrs={'data-occludable-job-id': True})\n",
    "\n",
    "# #     for li in job_post_lis:\n",
    "# #         job_post_ids.append(li[\"data-occludable-job-id\"])\n",
    "\n",
    "\n",
    "# page = driver.page_source\n",
    " \n",
    "# soup = BeautifulSoup(page, 'lxml')\n",
    "# job_post_divs = soup.find_all('div', attrs={\"data-view-name\": \"job-card\"})\n",
    "\n",
    "# for div in job_post_divs:\n",
    "#     job_post_ids.add(div[\"data-job-id\"])\n",
    "\n",
    "# job_post_lis = soup.find_all('li', attrs={'data-occludable-job-id': True})\n",
    "\n",
    "# for li in job_post_lis:\n",
    "#     job_post_ids.add(li[\"data-occludable-job-id\"])\n",
    "\n",
    "# print(len(job_post_ids), job_post_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n"
     ]
    }
   ],
   "source": [
    "# calculate safety:\n",
    "cal_safety=((461-25)//25)*25\n",
    "if max_num == 0:\n",
    "    num_to_scrape = cal_safety\n",
    "else:\n",
    "    if (0<max_num) and (max_num<cal_safety):\n",
    "        num_to_scrape = max_num\n",
    "    else:\n",
    "        print(\"Invalid max_num supplied, using cal_safety\")\n",
    "        num_to_scrape = cal_safety\n",
    "\n",
    "for i in range(0, num_to_scrape, 25):\n",
    "    _base_url = os.getenv(\"USER_PROVIDED_URL\")\n",
    "    _postfix = f\"&start={i}\"\n",
    "    page_url = _base_url + _postfix\n",
    "\n",
    "    if i == 0:\n",
    "        driver.get(_base_url)\n",
    "    else:\n",
    "        driver.get(page_url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    page = driver.page_source\n",
    "    \n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    job_post_divs = soup.find_all('div', attrs={\"data-view-name\": \"job-card\"})\n",
    "\n",
    "    for div in job_post_divs:\n",
    "        job_post_ids.add(div[\"data-job-id\"])\n",
    "\n",
    "    job_post_lis = soup.find_all('li', attrs={'data-occludable-job-id': True})\n",
    "\n",
    "    for li in job_post_lis:\n",
    "        job_post_ids.add(li[\"data-occludable-job-id\"])\n",
    "\n",
    "    print(len(job_post_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _base_url = os.getenv(\"USER_PROVIDED_URL\")\n",
    "# _postfix = \"&start=25\"\n",
    "# page_url = _base_url + _postfix\n",
    "\n",
    "# driver.get(page_url)\n",
    "# time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with open('first_page.html', 'r', encoding='utf-8') as f:\n",
    "\n",
    "# #     contents = f.read()\n",
    "\n",
    "# #     soup = BeautifulSoup(contents, 'lxml')\n",
    "# #     job_post_divs = soup.find_all('div', attrs={\"data-view-name\": \"job-card\"})\n",
    "\n",
    "# #     for div in job_post_divs:\n",
    "# #         job_post_ids.append(div[\"data-job-id\"])\n",
    "\n",
    "# #     job_post_lis = soup.find_all('li', attrs={'data-occludable-job-id': True})\n",
    "\n",
    "# #     for li in job_post_lis:\n",
    "# #         job_post_ids.append(li[\"data-occludable-job-id\"])\n",
    "\n",
    "# ###\n",
    "\n",
    "# page = driver.page_source\n",
    " \n",
    "# soup = BeautifulSoup(page, 'lxml')\n",
    "# job_post_divs = soup.find_all('div', attrs={\"data-view-name\": \"job-card\"})\n",
    "\n",
    "# for div in job_post_divs:\n",
    "#     job_post_ids.add(div[\"data-job-id\"])\n",
    "\n",
    "# job_post_lis = soup.find_all('li', attrs={'data-occludable-job-id': True})\n",
    "\n",
    "# for li in job_post_lis:\n",
    "#     job_post_ids.add(li[\"data-occludable-job-id\"])\n",
    "\n",
    "# print(len(job_post_ids), job_post_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelvin\\AppData\\Local\\Temp\\ipykernel_20572\\2827516010.py:1: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for id in tqdm(job_post_ids, desc='completed job_ids'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122e688da4014b5084375afa7901354d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "completed job_ids:   0%|          | 0/425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping job_id: 3771842589; duplicated/ found in save slate\n",
      "skipping job_id: 3789945931; duplicated/ found in save slate\n",
      "skipping job_id: 3791409552; duplicated/ found in save slate\n",
      "skipping job_id: 3766022721; duplicated/ found in save slate\n",
      "skipping job_id: 3794784458; duplicated/ found in save slate\n",
      "skipping job_id: 3774303768; duplicated/ found in save slate\n",
      "skipping job_id: 3669874647; duplicated/ found in save slate\n",
      "skipping job_id: 3738246097; duplicated/ found in save slate\n",
      "skipping job_id: 3788272820; duplicated/ found in save slate\n",
      "skipping job_id: 3736318809; duplicated/ found in save slate\n",
      "skipping job_id: 3787800069; duplicated/ found in save slate\n",
      "skipping job_id: 3674404146; duplicated/ found in save slate\n",
      "skipping job_id: 3745348244; duplicated/ found in save slate\n",
      "skipping job_id: 3728589956; duplicated/ found in save slate\n",
      "skipping job_id: 3775444318; duplicated/ found in save slate\n",
      "skipping job_id: 3669875571; duplicated/ found in save slate\n",
      "skipping job_id: 3784377594; duplicated/ found in save slate\n",
      "skipping job_id: 3758134025; duplicated/ found in save slate\n",
      "skipping job_id: 3741489382; duplicated/ found in save slate\n",
      "skipping job_id: 3789289758; duplicated/ found in save slate\n",
      "skipping job_id: 3763136916; duplicated/ found in save slate\n",
      "skipping job_id: 3688870240; duplicated/ found in save slate\n",
      "skipping job_id: 3724274874; duplicated/ found in save slate\n",
      "skipping job_id: 3776184007; duplicated/ found in save slate\n",
      "skipping job_id: 3677324427; duplicated/ found in save slate\n",
      "skipping job_id: 3782018680; duplicated/ found in save slate\n",
      "skipping job_id: 3720985854; duplicated/ found in save slate\n",
      "skipping job_id: 3768307950; duplicated/ found in save slate\n",
      "skipping job_id: 3785093198; duplicated/ found in save slate\n",
      "skipping job_id: 3782021313; duplicated/ found in save slate\n",
      "skipping job_id: 3724278418; duplicated/ found in save slate\n",
      "skipping job_id: 3770505963; duplicated/ found in save slate\n",
      "skipping job_id: 3774303687; duplicated/ found in save slate\n",
      "skipping job_id: 3599182645; duplicated/ found in save slate\n",
      "skipping job_id: 3784202883; duplicated/ found in save slate\n",
      "skipping job_id: 3782021314; duplicated/ found in save slate\n",
      "skipping job_id: 3793613953; duplicated/ found in save slate\n",
      "skipping job_id: 3772219086; duplicated/ found in save slate\n",
      "skipping job_id: 3775439945; duplicated/ found in save slate\n",
      "skipping job_id: 3701158703; duplicated/ found in save slate\n",
      "skipping job_id: 3771458958; duplicated/ found in save slate\n",
      "skipping job_id: 3783397927; duplicated/ found in save slate\n",
      "skipping job_id: 3744941004; duplicated/ found in save slate\n",
      "skipping job_id: 3782764414; duplicated/ found in save slate\n",
      "skipping job_id: 3792801002; duplicated/ found in save slate\n",
      "skipping job_id: 3758460602; duplicated/ found in save slate\n",
      "skipping job_id: 3763881426; duplicated/ found in save slate\n",
      "skipping job_id: 3767698415; duplicated/ found in save slate\n",
      "skipping job_id: 3766325180; duplicated/ found in save slate\n",
      "skipping job_id: 3745518697; duplicated/ found in save slate\n",
      "skipping job_id: 3789712364; duplicated/ found in save slate\n",
      "skipping job_id: 3781001789; duplicated/ found in save slate\n",
      "skipping job_id: 3792043634; duplicated/ found in save slate\n",
      "skipping job_id: 3738247233; duplicated/ found in save slate\n",
      "skipping job_id: 3793278953; duplicated/ found in save slate\n",
      "skipping job_id: 3668305504; duplicated/ found in save slate\n",
      "skipping job_id: 3701197719; duplicated/ found in save slate\n",
      "skipping job_id: 3726119674; duplicated/ found in save slate\n",
      "skipping job_id: 3792094898; duplicated/ found in save slate\n",
      "skipping job_id: 3735584240; duplicated/ found in save slate\n",
      "skipping job_id: 3793892955; duplicated/ found in save slate\n",
      "skipping job_id: 3791607601; duplicated/ found in save slate\n",
      "skipping job_id: 3782227000; duplicated/ found in save slate\n",
      "skipping job_id: 3784576198; duplicated/ found in save slate\n",
      "skipping job_id: 3782387384; duplicated/ found in save slate\n",
      "skipping job_id: 3792335772; duplicated/ found in save slate\n",
      "skipping job_id: 3788274613; duplicated/ found in save slate\n",
      "skipping job_id: 3744792120; duplicated/ found in save slate\n",
      "skipping job_id: 3782760110; duplicated/ found in save slate\n",
      "skipping job_id: 3766330446; duplicated/ found in save slate\n",
      "skipping job_id: 3765035699; duplicated/ found in save slate\n",
      "skipping job_id: 3787829617; duplicated/ found in save slate\n",
      "skipping job_id: 3793618353; duplicated/ found in save slate\n",
      "skipping job_id: 3771461550; duplicated/ found in save slate\n",
      "skipping job_id: 3783145684; duplicated/ found in save slate\n",
      "skipping job_id: 3768271648; duplicated/ found in save slate\n",
      "skipping job_id: 3787858840; duplicated/ found in save slate\n",
      "skipping job_id: 3744788471; duplicated/ found in save slate\n",
      "skipping job_id: 3744793070; duplicated/ found in save slate\n",
      "skipping job_id: 3318540515; duplicated/ found in save slate\n",
      "skipping job_id: 3783906670; duplicated/ found in save slate\n",
      "skipping job_id: 3741967340; duplicated/ found in save slate\n",
      "skipping job_id: 3787295550; duplicated/ found in save slate\n",
      "skipping job_id: 3744787582; duplicated/ found in save slate\n",
      "skipping job_id: 3735055058; duplicated/ found in save slate\n",
      "skipping job_id: 3771456823; duplicated/ found in save slate\n",
      "skipping job_id: 3783150307; duplicated/ found in save slate\n",
      "skipping job_id: 3740539282; duplicated/ found in save slate\n",
      "skipping job_id: 3793897142; duplicated/ found in save slate\n",
      "skipping job_id: 3779142461; duplicated/ found in save slate\n",
      "skipping job_id: 3727898864; duplicated/ found in save slate\n",
      "skipping job_id: 3718951907; duplicated/ found in save slate\n",
      "skipping job_id: 3680012739; duplicated/ found in save slate\n",
      "skipping job_id: 3779362598; duplicated/ found in save slate\n",
      "skipping job_id: 3671494251; duplicated/ found in save slate\n",
      "skipping job_id: 3685599582; duplicated/ found in save slate\n",
      "skipping job_id: 3725446692; duplicated/ found in save slate\n",
      "skipping job_id: 3771473060; duplicated/ found in save slate\n",
      "skipping job_id: 3671493405; duplicated/ found in save slate\n"
     ]
    }
   ],
   "source": [
    "for id in tqdm(job_post_ids, desc='completed job_ids'):\n",
    "    if id in job_desc_id:\n",
    "        print(f\"skipping job_id: {id}; duplicated/ found in save slate\")\n",
    "        continue\n",
    "\n",
    "    driver.get(f\"https://www.linkedin.com/jobs/search/?currentJobId={id}\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "\n",
    "    job_title_element = soup.find('span', class_='job-details-jobs-unified-top-card__job-title-link')\n",
    "    job_title = job_title_element.text.strip()\n",
    "\n",
    "    job_post_divs = soup.find('div', attrs={\"id\": \"job-details\"})\n",
    "    # get text\n",
    "    job_post_div_texts = job_post_divs.get_text()\n",
    "    # replace non-ascii with space\n",
    "    p1_job_desc = ''.join([i if ord(i) < 128 else ' ' for i in job_post_div_texts])\n",
    "    # convert all to lowercase\n",
    "    p2_job_desc = p1_job_desc.lower()\n",
    "    # p1_job_desc = job_post_divs.get_text().encode('ascii', errors='ignore').decode().lower()\n",
    "    p3_job_desc = \" \".join(p2_job_desc.split(r\"\\n\")).replace(\"about the job\", \" \").replace(\"this job is sourced from a job board.\", \" \")\n",
    "    # merge consecutive spaces\n",
    "    p4_job_desc = \" \".join(p3_job_desc.split())\n",
    "    # merge consecutive spaces\n",
    "    pf_job_desc = r\"\\n\".join(p4_job_desc.split(r\"\\n\"))\n",
    "\n",
    "    job_desc_id[id] = {\n",
    "        \"job_title\": job_title,\n",
    "        \"job_desc\": pf_job_desc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_desc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_json, \"w\") as outfile:\n",
    "    json.dump(job_desc_id, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "job_scrapers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
